#!/usr/bin/env python
import sys
import logging

from xformer import loading

from tak.proto import analysis_pb2_grpc
from tak.proto import analysis_pb2
import argparse

import torch
from torch import nn

from attrs import define, field

import grpc
import asyncio
import time

import typing as T

MAX_QUEUE_DEPTH = 80


@define
class QueueRequest:
    position: torch.Tensor

    ready: asyncio.Event = field(factory=asyncio.Event)
    probs: T.Optional[torch.Tensor] = None
    value: T.Optional[torch.Tensor] = None


@define
class Server(analysis_pb2_grpc.AnalysisServicer):
    model: nn.Module
    device: str = "cpu"

    queue: asyncio.Queue = field(factory=lambda: asyncio.Queue(MAX_QUEUE_DEPTH))

    async def worker_loop(self):
        # loop = asyncio.get_running_loop()

        while True:
            batch = []
            batch.append(await self.queue.get())
            while True:
                if len(batch) >= 8:
                    try:
                        batch.append(self.queue.get_nowait())
                    except asyncio.QueueEmpty:
                        break
                else:
                    try:
                        elem = await asyncio.wait_for(self.queue.get(), 1.0 / 1000)
                        batch.append(elem)
                    except asyncio.TimeoutError:
                        break
            # we have a batch

            positions = torch.zeros(
                (len(batch), max(len(b.position) for b in batch)), dtype=torch.long
            )
            mask = torch.zeros_like(positions, dtype=torch.bool)
            for (i, b) in enumerate(batch):
                positions[i, : len(b.position)] = b.position
                mask[i, len(b.position) :].fill_(1)

            def run_model():
                out = self.model(positions.to(self.device), mask.to(self.device))
                probs = torch.softmax(out["moves"], dim=-1).cpu()
                return (probs, out["values"].cpu())

            start = time.perf_counter()
            # (probs, values) = await loop.run_in_executor(None, run_model)
            (probs, values) = run_model()
            end = time.perf_counter()
            logging.info(
                f"did batch len={positions.size(0)} dur={1000*(end-start):0.1f}ms"
            )
            for (i, b) in enumerate(batch):
                b.probs = probs[i]
                b.value = values[i].item()
                b.ready.set()

    async def Evaluate(self, request, context):
        position = torch.tensor(request.position, device=self.device, dtype=torch.long)

        req = QueueRequest(position=position)
        await self.queue.put(req)
        await req.ready.wait()

        return analysis_pb2.EvaluateResponse(
            move_probs=req.probs.tolist(), value=req.value
        )


_cleanup_coroutines = []


async def main(argv):
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--graph",
        action="store_true",
        default=False,
        help="Use CUDA graphs to run the network",
    )
    parser.add_argument(
        "--fp16",
        action="store_true",
        default=False,
        help="Run model in float16",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
    )
    parser.add_argument(
        "--host",
        type=str,
        default="localhost",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=5001,
    )
    parser.add_argument(
        "model",
        type=str,
    )

    args = parser.parse_args(argv)

    model = loading.load_model(args.model, args.device)
    if args.fp16:
        model = model.to(torch.float16)
    model = torch.jit.script(model)

    server = grpc.aio.server()
    server.add_insecure_port(f"{args.host}:{args.port}")

    analysis = Server(model=model, device=args.device)
    worker = asyncio.create_task(analysis.worker_loop())

    analysis_pb2_grpc.add_AnalysisServicer_to_server(
        analysis,
        server,
    )
    await server.start()

    async def server_graceful_shutdown():
        logging.info("Starting graceful shutdown...")
        # Shuts down the server with 5 seconds of grace period. During the
        # grace period, the server won't accept new connections and allow
        # existing RPCs to continue within the grace period.
        await server.stop(2)
        worker.cancel()

    _cleanup_coroutines.append(server_graceful_shutdown())

    await server.wait_for_termination()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    loop = asyncio.get_event_loop()
    try:
        loop.run_until_complete(main(sys.argv[1:]))
    finally:
        for co in _cleanup_coroutines:
            loop.run_until_complete(co)
        loop.close()
